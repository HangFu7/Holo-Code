import argparse
import torch
import numpy as np
import os
from tqdm import tqdm
from diffusers import DPMSolverMultistepScheduler
from inverse_stable_diffusion import InversableStableDiffusionPipeline
from torchvision import transforms
from datasets import load_dataset 

# å¯¼å…¥ä½ æä¾›çš„ image_distortion
from image_utils import image_distortion

# å¯¼å…¥ä¿®æ”¹åçš„ Holo_Shading
from watermark import Holo_Shading 

# ==========================================
# 0. é…ç½®æ”»å‡»å‚æ•°åˆ—è¡¨
# ==========================================
CROP_RATIOS = [0.7, 0.6, 0.5, 0.4, 0.3]
JPEG_QUALS  = [50, 40, 30, 20, 10]

# ==========================================
# 1. å†…ç½® Dataset åŠ è½½å‡½æ•°
# ==========================================
def get_dataset(args):
    print(f"Loading dataset: {args.dataset_path}...")
    try:
        dataset = load_dataset(args.dataset_path)['train']
        prompt_key = 'text'
        return dataset, prompt_key
    except Exception as e:
        print(f"Warning: Failed to load dataset '{args.dataset_path}': {e}")
        print("Using dummy prompts for testing instead.")
        dummy_data = [{'text': 'a beautiful landscape with a mountain and a lake'} for _ in range(args.num)]
        return dummy_data, 'text'

# ==========================================
# 2. è¾…åŠ©ç±»ï¼šæ¨¡æ‹Ÿ Args
# ==========================================
class AttackArgs:
    def __init__(self):
        self.jpeg_ratio = None
        self.random_crop_ratio = None
        self.random_drop_ratio = None
        self.gaussian_blur_r = None
        self.median_blur_k = None
        self.resize_ratio = None
        self.gaussian_std = None
        self.sp_prob = None
        self.brightness_factor = None
        self.contrast_factor = None
        self.translation_shift = None
        self.perspective_scale = None
        self.crop_scale_ratio = None
        self.composite_crop_jpeg = None
        self.vae_attack = False
        self.vae_quality = 3

# ==========================================
# 3. è¾…åŠ©å‡½æ•°ï¼šå›¾åƒé¢„å¤„ç†
# ==========================================
def transform_img_tensor(image, target_size=512):
    tform = transforms.Compose([
        transforms.Resize(target_size),
        transforms.CenterCrop(target_size),
        transforms.ToTensor(),
    ])
    image = tform(image)
    return 2.0 * image - 1.0

# ==========================================
# 4. æ ¸å¿ƒæ¶ˆèé€»è¾‘
# ==========================================
def run_ablation(args):
    # --- åˆå§‹åŒ– Pipeline ---
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Loading Model: {args.model_path}...")
    
    scheduler = DPMSolverMultistepScheduler.from_pretrained(args.model_path, subfolder='scheduler')
    pipe = InversableStableDiffusionPipeline.from_pretrained(
        args.model_path,
        scheduler=scheduler,
        torch_dtype=torch.float16,
    ).to(device)
    pipe.safety_checker = None

    # --- å‡†å¤‡ Inversion ç”¨çš„ Embedding ---
    null_prompt = ''
    null_embeddings = pipe.get_text_embedding(null_prompt)

    # --- åŠ è½½æ•°æ®é›† ---
    dataset, prompt_key = get_dataset(args)

    # --- å®šä¹‰å¯¹æ¯”æ¨¡å¼ (åªè·‘ä¸¤ä¸ªæ¶ˆèæ–¹æ¡ˆ) ---
    modes = ['no_holo', 'no_ecc']
    
    # ç»“æœå®¹å™¨ç»“æ„: mode -> type -> param -> [acc_list]
    results = {}
    for m in modes:
        results[m] = {
            'crop': {r: [] for r in CROP_RATIOS},
            'jpeg': {q: [] for q in JPEG_QUALS}
        }

    print(f"\n{'='*60}")
    print(f"ğŸ”¬ Running Ablation (No Holo vs No ECC)")
    print(f"   Samples: {args.num}")
    print(f"   Crop Ratios: {CROP_RATIOS}")
    print(f"   JPEG Qualities: {JPEG_QUALS}")
    print(f"{'='*60}\n")

    # --- å¾ªç¯æµ‹è¯• ---
    for mode in modes:
        print(f"ğŸ‘‰ Testing Variant: [{mode.upper()}]")
        
        # åˆå§‹åŒ– Holo_Shading
        watermark = Holo_Shading(
            args.channel_copy, 
            args.hw_copy, 
            args.fpr, 
            args.user_number, 
            mode=mode 
        )

        for i in tqdm(range(args.num), desc=f"  Evaluating {mode}"):
            seed = args.gen_seed + i
            idx = i % len(dataset)
            current_prompt = dataset[idx][prompt_key]
            
            # 1. ç”Ÿæˆ (Generation) - æ¯å¼ å›¾åªç”Ÿæˆä¸€æ¬¡ï¼Œç„¶ååº”ç”¨æ‰€æœ‰æ”»å‡»
            init_latents_w = watermark.create_watermark_and_return_w()
            
            with torch.no_grad():
                outputs = pipe(
                    current_prompt,
                    num_images_per_prompt=1,
                    guidance_scale=args.guidance_scale, 
                    num_inference_steps=args.num_inference_steps, 
                    height=args.image_length, 
                    width=args.image_length, 
                    latents=init_latents_w, 
                )
            image_w = outputs.images[0] # PIL Image

            # 2. å¾ªç¯æµ‹è¯•æ‰€æœ‰ Crop å‚æ•°
            for ratio in CROP_RATIOS:
                args_crop = AttackArgs()
                args_crop.random_crop_ratio = ratio
                img_crop = image_distortion(image_w.copy(), seed, args_crop)
                
                acc = invert_and_eval(pipe, watermark, img_crop, null_embeddings, args.num_inversion_steps, device)
                results[mode]['crop'][ratio].append(acc)

            # 3. å¾ªç¯æµ‹è¯•æ‰€æœ‰ JPEG å‚æ•°
            for qual in JPEG_QUALS:
                args_jpeg = AttackArgs()
                args_jpeg.jpeg_ratio = qual
                img_jpeg = image_distortion(image_w.copy(), seed, args_jpeg)
                
                acc = invert_and_eval(pipe, watermark, img_jpeg, null_embeddings, args.num_inversion_steps, device)
                results[mode]['jpeg'][qual].append(acc)

    # --- æ‰“å°ç»“æœ ---
    print_detailed_results(results, modes)

def invert_and_eval(pipe, watermark, image, text_embeddings, steps, device):
    """ æ‰§è¡Œ Inversion å¹¶æå–æ°´å° """
    img_tensor = transform_img_tensor(image).unsqueeze(0).to(text_embeddings.dtype).to(device)
    
    # Encode -> Inversion
    image_latents = pipe.get_image_latents(img_tensor, sample=False)
    reversed_latents = pipe.forward_diffusion(
        latents=image_latents,
        text_embeddings=text_embeddings, 
        guidance_scale=1, 
        num_inference_steps=steps, 
    )
    
    # Decode
    res = watermark.eval_watermark(reversed_latents)
    if isinstance(res, tuple):
        return res[0] # åªå– Bit Accuracy
    return res

def print_detailed_results(results, modes):
    # æ˜ å°„åç§°æ–¹ä¾¿é˜…è¯»
    name_map = {
        'no_holo': 'No Holo (Variant A)',
        'no_ecc':  'No ECC  (Variant B)'
    }

    print("\n" + "="*60)
    print("                ABLATION STUDY RESULTS")
    print("="*60)

    # --- Print JPEG Table ---
    print("\n[1] JPEG Compression Robustness (Bit Acc)")
    header = f"{'Metric':<10} | " + " | ".join([f"Q={q:<3}" for q in JPEG_QUALS])
    print("-" * len(header))
    print(header)
    print("-" * len(header))
    
    for mode in modes:
        row_str = f"{name_map[mode]:<10} | "
        accs = []
        for q in JPEG_QUALS:
            avg_acc = np.mean(results[mode]['jpeg'][q])
            accs.append(f"{avg_acc:.4f}")
        print(row_str + " | ".join(accs))
    
    # --- Print Crop Table ---
    print("\n[2] Random Crop Robustness (Bit Acc)")
    header = f"{'Metric':<10} | " + " | ".join([f"R={r:<3}" for r in CROP_RATIOS])
    print("-" * len(header))
    print(header)
    print("-" * len(header))
    
    for mode in modes:
        row_str = f"{name_map[mode]:<10} | "
        accs = []
        for r in CROP_RATIOS:
            avg_acc = np.mean(results[mode]['crop'][r])
            accs.append(f"{avg_acc:.4f}")
        print(row_str + " | ".join(accs))
        
    print("="*60)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    
    parser.add_argument('--num', default=50, type=int, help="Sample count per variant")
    parser.add_argument('--model_path', default='./stable-diffusion-2-1-base')
    parser.add_argument('--dataset_path', default='./Stable-Diffusion-Prompts')
    parser.add_argument('--output_path', default='./output_ablation')
    
    parser.add_argument('--image_length', default=512, type=int)
    parser.add_argument('--guidance_scale', default=7.5, type=float)
    parser.add_argument('--num_inference_steps', default=50, type=int)
    parser.add_argument('--num_inversion_steps', default=50, type=int)
    parser.add_argument('--gen_seed', default=2024, type=int)
    
    parser.add_argument('--channel_copy', default=1, type=int)
    parser.add_argument('--hw_copy', default=6, type=int, help="Block size for ablation")
    parser.add_argument('--fpr', default=1e-6, type=float)
    parser.add_argument('--user_number', default=1000000, type=int)
    
    args = parser.parse_args()
    os.makedirs(args.output_path, exist_ok=True)
    
    run_ablation(args)