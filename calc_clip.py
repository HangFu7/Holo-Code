import argparse
import os
import json
import torch
from PIL import Image
from tqdm import tqdm
import open_clip

def main(args):
    print(f"ğŸš€ Starting CLIP Evaluation for: {args.image_folder}")
    
    # 1. è®¾ç½®è®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # 2. åŠ è½½æœ¬åœ° CLIP æ¨¡å‹
    # ä½ çš„æ¨¡å‹è·¯å¾„: ./clip-vit-g-14/open_clip_pytorch_model.bin
    print(f"Loading CLIP model: {args.model_name}...")
    print(f"Weights path: {args.pretrained_path}")
    
    try:
        model, _, preprocess = open_clip.create_model_and_transforms(
            args.model_name, 
            pretrained=args.pretrained_path, 
            device=device
        )
    except Exception as e:
        print(f"âŒ Model load failed: {e}")
        print("Tip: Ensure you renamed the local 'open_clip' folder to 'open_clip_legacy'!")
        return

    tokenizer = open_clip.get_tokenizer(args.model_name)
    model.eval()

    # 3. åŠ è½½ Prompts (meta_data.json)
    print(f"Loading prompts from {args.json_path}...")
    with open(args.json_path, 'r') as f:
        data = json.load(f)
        image_list = data['images']
        annotation_list = data['annotations']

    scores = []
    
    # 4. éå†å¹¶è®¡ç®—
    print("Calculating scores...")
    for i in tqdm(range(len(image_list))):
        # A. è·å– Prompt å’Œ æ–‡ä»¶å
        prompt = annotation_list[i]['caption']
        file_name = image_list[i]['file_name']
        
        # B. å¯¹é½æ–‡ä»¶å (.jpg -> .png)
        # FID å®éªŒç”Ÿæˆçš„å›¾ç‰‡éƒ½æ˜¯ .png ç»“å°¾
        file_name = file_name.replace('.jpg', '.png')
        image_path = os.path.join(args.image_folder, file_name)
        
        # C. æ£€æŸ¥å›¾ç‰‡æ˜¯å¦å­˜åœ¨ (é˜²æ­¢ FID è¿˜æ²¡è·‘å®ŒæŠ¥é”™)
        if not os.path.exists(image_path):
            continue

        # D. è®¡ç®—åˆ†æ•°
        try:
            image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)
            text = tokenizer([prompt]).to(device)

            with torch.no_grad(), torch.cuda.amp.autocast():
                image_features = model.encode_image(image)
                text_features = model.encode_text(text)
                
                # å½’ä¸€åŒ–
                image_features /= image_features.norm(dim=-1, keepdim=True)
                text_features /= text_features.norm(dim=-1, keepdim=True)

                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                score = (image_features @ text_features.T).item()
                scores.append(score)
        except Exception as e:
            pass

    # 5. ç»Ÿè®¡ä¸ä¿å­˜
    if len(scores) > 0:
        mean_score = sum(scores) / len(scores)
        print(f"\n{'='*40}")
        print(f"âœ… Folder: {os.path.basename(args.image_folder)}")
        print(f"ğŸ“Š Mean CLIP Score: {mean_score:.4f}")
        print(f"ğŸ–¼ï¸ Images Processed: {len(scores)}")
        print(f"{'='*40}\n")
        
        # è¿½åŠ å†™å…¥æ€»ç»“æœæ–‡ä»¶
        with open("final_clip_results.txt", "a") as f:
            f.write(f"Experiment: {args.run_name} | Score: {mean_score:.4f}\n")
    else:
        print("âŒ No images found. Check path or wait for FID generation.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--image_folder", type=str, required=True)
    parser.add_argument("--run_name", type=str, required=True, help="Label for the result txt")
    parser.add_argument("--json_path", type=str, default="./fid_outputs/coco/meta_data.json")
    
    # CLIP é…ç½®
    parser.add_argument("--model_name", type=str, default="ViT-g-14")
    # æŒ‡å‘ä½ åˆšæ‰ ls çœ‹åˆ°çš„ .bin æ–‡ä»¶è·¯å¾„
    parser.add_argument("--pretrained_path", type=str, default="./clip-vit-g-14/open_clip_pytorch_model.bin")
    
    args = parser.parse_args()
    main(args)